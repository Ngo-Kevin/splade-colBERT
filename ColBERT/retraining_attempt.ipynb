{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3bfeab6-4c13-4461-b5cf-2077ac1d0155",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb623ab-d2c2-41cb-827d-4a711b68a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0957f2-0875-4c29-99ea-da153322fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsize = batch size\n",
    "config = ColBERTConfig(\n",
    "        bsize=64,\n",
    "        root=r\"../retrain_colbert\",\n",
    "        nbits=2,\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab2d642-efbd-487c-9d50-d12b36ff7a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Starting...\n",
      "#> Joined...\n",
      "Saved checkpoint to None...\n"
     ]
    }
   ],
   "source": [
    "# nranks = number of gpu\n",
    "with Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\n",
    "    trainer = Trainer(\n",
    "        triples= r\"../data/triples.train.small.id.json\",\n",
    "        queries= r\"../data/queries.train.tsv\",\n",
    "        collection= r\"../data/collection.tsv\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    checkpoint_path = trainer.train()\n",
    "\n",
    "    print(f\"Saved checkpoint to {checkpoint_path}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001cd55-7cf9-46a8-8c3c-632e581c8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a9c00-5745-46bc-9555-0979d1c011f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Trying to Load Torch Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d39c67d-4838-4fa7-9e39-ccb788075799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "567119de-99c5-4dfb-b60a-7edc5b7dbf55",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m decompress_residuals_cpp \u001b[38;5;241m=\u001b[39m load(\n\u001b[1;32m      2\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecompress_residuals_cpp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     sources\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdecompress_residuals \u001b[38;5;241m=\u001b[39m decompress_residuals_cpp\u001b[38;5;241m.\u001b[39mdecompress_residuals_cpp\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cls' is not defined"
     ]
    }
   ],
   "source": [
    "decompress_residuals_cpp = load(\n",
    "    name=\"decompress_residuals_cpp\",\n",
    "    sources=[\n",
    "        r'colbert/indexing/codecs/decompress_residuals.cpp',\n",
    "        r\"colbert/indexing/codecs/decompress_residuals.cu\",\n",
    "    ],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0bd2a36-4882-4bb8-9aa0-4c2a276d3ff2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing packbits_cpp: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m packbits_cpp \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpackbits_cpp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDocuments\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPython_Scripts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUCB\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msplade-colBERT\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mColBERT\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcolbert\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mindexing\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcodecs\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpackbits.cpp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDocuments\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPython_Scripts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUCB\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msplade-colBERT\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mColBERT\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcolbert\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mindexing\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcodecs\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpackbits.cu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ir\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1202\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(name,\n\u001b[0;32m   1112\u001b[0m          sources: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   1113\u001b[0m          extra_cflags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1121\u001b[0m          is_standalone\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1122\u001b[0m          keep_intermediates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1123\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;124;03m    Loads a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;124;03m                verbose=True)\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m-> 1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msources\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_get_build_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ir\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1450\u001b[0m, in \u001b[0;36m_jit_compile\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_standalone:\n\u001b[0;32m   1448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_exec_path(name, build_directory)\n\u001b[1;32m-> 1450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_import_module_from_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ir\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1844\u001b[0m, in \u001b[0;36m_import_module_from_library\u001b[1;34m(module_name, path, is_python_module)\u001b[0m\n\u001b[0;32m   1842\u001b[0m spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mspec_from_file_location(module_name, filepath)\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1844\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_from_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, importlib\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mLoader)\n\u001b[0;32m   1846\u001b[0m spec\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39mexec_module(module)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:565\u001b[0m, in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1173\u001b[0m, in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing packbits_cpp: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "packbits_cpp = load(\n",
    "    name=\"packbits_cpp\",\n",
    "    sources=[\n",
    "        r'D:\\Documents\\Python_Scripts\\class\\UCB\\splade-colBERT\\ColBERT\\colbert\\indexing\\codecs\\packbits.cpp',\n",
    "        r'D:\\Documents\\Python_Scripts\\class\\UCB\\splade-colBERT\\ColBERT\\colbert\\indexing\\codecs\\packbits.cu',\n",
    "    ],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb39dd-46cc-4274-8a2d-5b719617825c",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1020cce3-3e7f-4fda-bc63-432803396c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4be46a04-f223-4e73-8ddb-1cae8b862b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsize = batch size\n",
    "config = ColBERTConfig(\n",
    "        bsize = 64,\n",
    "        resume = False,\n",
    "        checkpoint = \"experiments/msmarco/colbert\",\n",
    "        collection = \"../data/collection.tsv\",\n",
    "        index_name = \"msmarco.nbits=2\",\n",
    "        root = \"experiments\",\n",
    "        experiment = \"msmarco\",\n",
    "        name = \"\",\n",
    "        rank = 0,\n",
    "        nranks = 1,\n",
    "        amp = True,\n",
    "        gpus = 1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "363af98e-ed88-4ae0-a5fa-9f2d406b402a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Oct 08, 02:05:14] #> Note: Output directory /home/ubuntu/splade-colBERT/ColBERT/experiments/msmarco/indexes/msmarco.nbits=2 already exists\n",
      "\n",
      "\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": true,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 220,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"experiments\\/msmarco\\/colbert\",\n",
      "    \"triples\": \"..\\/data\\/triples.train.small.id.json\",\n",
      "    \"collection\": \"..\\/data\\/collection.tsv\",\n",
      "    \"queries\": \"..\\/data\\/queries.train.tsv\",\n",
      "    \"index_name\": \"msmarco.nbits=2\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/ubuntu\\/splade-colBERT\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"msmarco\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2022-10\\/08\\/01.51.47\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1\n",
      "}\n",
      "[Oct 08, 02:05:16] #> Loading collection...\n",
      "0M 1M 2M 3M 4M 5M 6M 7M 8M \n",
      "[Oct 08, 02:05:33] [0] \t\t #> Loaded plan from /home/ubuntu/splade-colBERT/ColBERT/experiments/msmarco/indexes/msmarco.nbits=2/plan.json:\n",
      "[Oct 08, 02:05:33] [0] \t\t #> num_chunks = 354\n",
      "[Oct 08, 02:05:33] [0] \t\t #> num_partitions = 354\n",
      "[Oct 08, 02:05:33] [0] \t\t #> num_embeddings_est = 597604280.772728\n",
      "[Oct 08, 02:05:33] [0] \t\t #> avg_doclen_est = 67.5883560180664\n",
      "[Oct 08, 02:05:33] [0] \t\t #> num_sample_embs = tensor([35225225], device='cuda:0')\n",
      "[Oct 08, 02:05:33] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 08, 02:05:34] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 155.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 0 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 1 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 2 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 3 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 4 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 5 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 6 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 7 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 8 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 9 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 10 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 11 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 12 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 13 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 14 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 15 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 16 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 17 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 18 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 19 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 20 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 21 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 22 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 23 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 24 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 25 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 26 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 27 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 28 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 29 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 30 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 31 in the index already, skipping encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:00, 155.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 32 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 33 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 34 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 35 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 36 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 37 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 38 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 39 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 40 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 41 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 42 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 43 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 44 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 45 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 46 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 47 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 48 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 49 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 50 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 51 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 52 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 53 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 54 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 55 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 56 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 57 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 58 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 59 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 60 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 61 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 62 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 63 in the index already, skipping encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:00, 155.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 64 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 65 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 66 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 67 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 68 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 69 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 70 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 71 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 72 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 73 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 74 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 75 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 76 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 77 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 78 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 79 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 80 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 81 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 82 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 83 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 84 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 85 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 86 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 87 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 88 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 89 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 90 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 91 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 92 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 93 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 94 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 95 in the index already, skipping encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:00, 156.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 96 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 97 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 98 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 99 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 100 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 101 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 102 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 103 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 104 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 105 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 106 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 107 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 108 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 109 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 110 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 111 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 112 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 113 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 114 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 115 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 116 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 117 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 118 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 119 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 120 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 121 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 122 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 123 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 124 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 125 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 126 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 127 in the index already, skipping encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [00:00, 156.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 128 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 129 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 130 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 131 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 132 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 133 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 134 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:21] [0] \t\t #> Found chunk 135 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 136 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 137 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 138 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 139 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 140 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 141 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 142 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 143 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 144 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 145 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 146 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 147 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 148 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 149 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 150 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 151 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 152 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 153 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 154 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 155 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 156 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 157 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 158 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 159 in the index already, skipping encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:01, 156.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 160 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 161 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 162 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 163 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 164 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 165 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 166 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 167 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 168 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 169 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 170 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 171 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 172 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 173 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 174 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 175 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 176 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 177 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 178 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 179 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 180 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 181 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 182 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 183 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 184 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 185 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 186 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 187 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 188 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 189 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 190 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 191 in the index already, skipping encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "208it [00:01, 156.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 192 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 193 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 194 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 195 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 196 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 197 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 198 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 199 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 200 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 201 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 202 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 203 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 204 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 205 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 206 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Found chunk 207 in the index already, skipping encoding...\n",
      "[Oct 08, 02:06:22] [0] \t\t #> Encoding 25000 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "208it [00:19, 156.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:08:03] [0] \t\t #> Saving chunk 208: \t 25,000 passages and 1,694,440 embeddings. From #5,200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209it [01:54,  2.99s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:08:15] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:10:06] [0] \t\t #> Saving chunk 209: \t 25,000 passages and 1,714,031 embeddings. From #5,225,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210it [03:59,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:10:20] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:12:12] [0] \t\t #> Saving chunk 210: \t 25,000 passages and 1,697,183 embeddings. From #5,250,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [06:04, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:12:25] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:14:18] [0] \t\t #> Saving chunk 211: \t 25,000 passages and 1,697,492 embeddings. From #5,275,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "212it [08:10, 20.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:14:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:16:24] [0] \t\t #> Saving chunk 212: \t 25,000 passages and 1,695,462 embeddings. From #5,300,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "213it [10:17, 29.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:16:38] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:18:34] [0] \t\t #> Saving chunk 213: \t 25,000 passages and 1,699,571 embeddings. From #5,325,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "214it [12:26, 40.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:18:47] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:20:43] [0] \t\t #> Saving chunk 214: \t 25,000 passages and 1,684,535 embeddings. From #5,350,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "215it [14:35, 52.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:20:56] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:22:48] [0] \t\t #> Saving chunk 215: \t 25,000 passages and 1,696,396 embeddings. From #5,375,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "216it [16:41, 64.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:23:02] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:24:55] [0] \t\t #> Saving chunk 216: \t 25,000 passages and 1,703,656 embeddings. From #5,400,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "217it [18:47, 76.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:25:08] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:27:04] [0] \t\t #> Saving chunk 217: \t 25,000 passages and 1,683,637 embeddings. From #5,425,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218it [20:55, 87.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:27:17] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:29:10] [0] \t\t #> Saving chunk 218: \t 25,000 passages and 1,754,417 embeddings. From #5,450,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "219it [23:02, 96.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:29:24] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:31:17] [0] \t\t #> Saving chunk 219: \t 25,000 passages and 1,745,696 embeddings. From #5,475,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "220it [25:10, 104.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:31:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:33:25] [0] \t\t #> Saving chunk 220: \t 25,000 passages and 1,752,768 embeddings. From #5,500,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [27:18, 110.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:33:39] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:35:33] [0] \t\t #> Saving chunk 221: \t 25,000 passages and 1,743,147 embeddings. From #5,525,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "222it [29:25, 115.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:35:46] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:37:40] [0] \t\t #> Saving chunk 222: \t 25,000 passages and 1,755,640 embeddings. From #5,550,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [31:33, 118.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:37:54] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:39:48] [0] \t\t #> Saving chunk 223: \t 25,000 passages and 1,754,330 embeddings. From #5,575,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "224it [33:41, 121.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:40:02] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:41:55] [0] \t\t #> Saving chunk 224: \t 25,000 passages and 1,753,298 embeddings. From #5,600,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [35:48, 122.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:42:09] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:44:03] [0] \t\t #> Saving chunk 225: \t 25,000 passages and 1,756,170 embeddings. From #5,625,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "226it [37:55, 124.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:44:17] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:46:10] [0] \t\t #> Saving chunk 226: \t 25,000 passages and 1,741,490 embeddings. From #5,650,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [40:03, 125.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:46:24] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:48:18] [0] \t\t #> Saving chunk 227: \t 25,000 passages and 1,759,731 embeddings. From #5,675,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "228it [42:10, 125.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:48:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:50:25] [0] \t\t #> Saving chunk 228: \t 25,000 passages and 1,749,718 embeddings. From #5,700,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "229it [44:18, 126.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:50:39] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:52:33] [0] \t\t #> Saving chunk 229: \t 25,000 passages and 1,725,894 embeddings. From #5,725,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "230it [46:25, 126.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:52:46] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:54:40] [0] \t\t #> Saving chunk 230: \t 25,000 passages and 1,759,034 embeddings. From #5,750,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231it [48:32, 126.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:54:54] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:56:51] [0] \t\t #> Saving chunk 231: \t 25,000 passages and 1,766,512 embeddings. From #5,775,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "232it [50:43, 128.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:57:04] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 02:58:58] [0] \t\t #> Saving chunk 232: \t 25,000 passages and 1,758,948 embeddings. From #5,800,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "233it [52:50, 127.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 02:59:11] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:01:05] [0] \t\t #> Saving chunk 233: \t 25,000 passages and 1,769,439 embeddings. From #5,825,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [54:58, 127.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:01:19] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:03:12] [0] \t\t #> Saving chunk 234: \t 25,000 passages and 1,761,697 embeddings. From #5,850,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "235it [57:05, 127.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:03:26] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:05:19] [0] \t\t #> Saving chunk 235: \t 25,000 passages and 1,747,700 embeddings. From #5,875,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "236it [59:12, 127.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:05:33] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:07:26] [0] \t\t #> Saving chunk 236: \t 25,000 passages and 1,767,343 embeddings. From #5,900,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "237it [1:01:19, 127.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:07:40] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:09:34] [0] \t\t #> Saving chunk 237: \t 25,000 passages and 1,723,522 embeddings. From #5,925,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "238it [1:03:26, 127.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:09:47] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:11:41] [0] \t\t #> Saving chunk 238: \t 25,000 passages and 1,721,576 embeddings. From #5,950,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "239it [1:05:33, 127.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:11:54] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:13:48] [0] \t\t #> Saving chunk 239: \t 25,000 passages and 1,745,373 embeddings. From #5,975,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [1:07:40, 127.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:14:01] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:15:55] [0] \t\t #> Saving chunk 240: \t 25,000 passages and 1,771,721 embeddings. From #6,000,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [1:09:48, 127.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:16:09] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:18:03] [0] \t\t #> Saving chunk 241: \t 25,000 passages and 1,775,884 embeddings. From #6,025,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "242it [1:11:56, 127.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:18:17] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:20:14] [0] \t\t #> Saving chunk 242: \t 25,000 passages and 1,766,922 embeddings. From #6,050,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "243it [1:14:07, 128.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:20:28] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:22:21] [0] \t\t #> Saving chunk 243: \t 25,000 passages and 1,734,175 embeddings. From #6,075,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [1:16:13, 127.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:22:34] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:24:28] [0] \t\t #> Saving chunk 244: \t 25,000 passages and 1,721,571 embeddings. From #6,100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [1:18:20, 127.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:24:42] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:26:38] [0] \t\t #> Saving chunk 245: \t 25,000 passages and 1,719,062 embeddings. From #6,125,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "246it [1:20:31, 128.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:26:52] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:28:46] [0] \t\t #> Saving chunk 246: \t 25,000 passages and 2,148,686 embeddings. From #6,150,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "247it [1:22:42, 129.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:29:03] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:30:58] [0] \t\t #> Saving chunk 247: \t 25,000 passages and 2,135,750 embeddings. From #6,175,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "248it [1:24:53, 129.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:31:15] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:33:09] [0] \t\t #> Saving chunk 248: \t 25,000 passages and 2,159,499 embeddings. From #6,200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "249it [1:27:05, 130.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:33:26] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:35:20] [0] \t\t #> Saving chunk 249: \t 25,000 passages and 2,159,308 embeddings. From #6,225,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [1:29:16, 130.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:35:37] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:37:31] [0] \t\t #> Saving chunk 250: \t 25,000 passages and 2,167,422 embeddings. From #6,250,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [1:31:27, 130.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:37:48] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:39:42] [0] \t\t #> Saving chunk 251: \t 25,000 passages and 2,159,789 embeddings. From #6,275,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "252it [1:33:38, 130.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:39:59] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:41:53] [0] \t\t #> Saving chunk 252: \t 25,000 passages and 2,056,133 embeddings. From #6,300,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "253it [1:35:48, 130.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:42:09] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:44:02] [0] \t\t #> Saving chunk 253: \t 25,000 passages and 1,747,154 embeddings. From #6,325,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "254it [1:37:55, 129.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:44:16] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:46:10] [0] \t\t #> Saving chunk 254: \t 25,000 passages and 1,747,149 embeddings. From #6,350,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "255it [1:40:03, 129.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:46:24] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:48:18] [0] \t\t #> Saving chunk 255: \t 25,000 passages and 1,745,865 embeddings. From #6,375,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [1:42:10, 128.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:48:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:50:25] [0] \t\t #> Saving chunk 256: \t 25,000 passages and 1,761,147 embeddings. From #6,400,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [1:44:18, 128.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:50:39] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:52:33] [0] \t\t #> Saving chunk 257: \t 25,000 passages and 1,760,319 embeddings. From #6,425,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "258it [1:46:26, 128.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:52:47] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:54:41] [0] \t\t #> Saving chunk 258: \t 25,000 passages and 1,764,948 embeddings. From #6,450,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "259it [1:48:33, 128.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:54:55] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:56:48] [0] \t\t #> Saving chunk 259: \t 25,000 passages and 1,758,369 embeddings. From #6,475,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "260it [1:50:41, 127.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:57:02] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 03:58:56] [0] \t\t #> Saving chunk 260: \t 25,000 passages and 1,757,615 embeddings. From #6,500,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "261it [1:52:48, 127.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 03:59:10] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:01:04] [0] \t\t #> Saving chunk 261: \t 25,000 passages and 1,772,969 embeddings. From #6,525,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "262it [1:54:56, 127.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:01:18] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:03:12] [0] \t\t #> Saving chunk 262: \t 25,000 passages and 1,742,227 embeddings. From #6,550,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "263it [1:57:04, 127.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:03:26] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:05:21] [0] \t\t #> Saving chunk 263: \t 25,000 passages and 1,740,813 embeddings. From #6,575,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "264it [1:59:14, 128.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:05:35] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:07:29] [0] \t\t #> Saving chunk 264: \t 25,000 passages and 1,769,861 embeddings. From #6,600,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "265it [2:01:21, 128.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:07:43] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:09:36] [0] \t\t #> Saving chunk 265: \t 25,000 passages and 1,756,548 embeddings. From #6,625,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "266it [2:03:29, 127.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:09:50] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:11:43] [0] \t\t #> Saving chunk 266: \t 25,000 passages and 1,741,370 embeddings. From #6,650,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [2:05:36, 127.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:11:57] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:13:50] [0] \t\t #> Saving chunk 267: \t 25,000 passages and 1,718,231 embeddings. From #6,675,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "268it [2:07:43, 127.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:14:04] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:15:57] [0] \t\t #> Saving chunk 268: \t 25,000 passages and 1,720,482 embeddings. From #6,700,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "269it [2:09:49, 127.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:16:10] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:18:04] [0] \t\t #> Saving chunk 269: \t 25,000 passages and 1,711,597 embeddings. From #6,725,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "270it [2:11:56, 127.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:18:17] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:20:11] [0] \t\t #> Saving chunk 270: \t 25,000 passages and 1,707,529 embeddings. From #6,750,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [2:14:03, 127.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:20:24] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:22:18] [0] \t\t #> Saving chunk 271: \t 25,000 passages and 1,711,045 embeddings. From #6,775,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272it [2:16:10, 127.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:22:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:24:25] [0] \t\t #> Saving chunk 272: \t 25,000 passages and 1,730,219 embeddings. From #6,800,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [2:18:17, 127.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:24:38] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:26:31] [0] \t\t #> Saving chunk 273: \t 25,000 passages and 1,714,809 embeddings. From #6,825,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [2:20:23, 126.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:26:45] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:28:38] [0] \t\t #> Saving chunk 274: \t 25,000 passages and 1,716,802 embeddings. From #6,850,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "275it [2:22:30, 126.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:28:51] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:30:45] [0] \t\t #> Saving chunk 275: \t 25,000 passages and 1,709,590 embeddings. From #6,875,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "276it [2:24:37, 126.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:30:58] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:32:54] [0] \t\t #> Saving chunk 276: \t 25,000 passages and 1,690,874 embeddings. From #6,900,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [2:26:46, 127.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:33:08] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:35:04] [0] \t\t #> Saving chunk 277: \t 25,000 passages and 1,710,894 embeddings. From #6,925,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "278it [2:28:56, 128.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:35:17] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:37:10] [0] \t\t #> Saving chunk 278: \t 25,000 passages and 1,706,163 embeddings. From #6,950,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "279it [2:31:02, 127.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:37:23] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:39:16] [0] \t\t #> Saving chunk 279: \t 25,000 passages and 1,706,319 embeddings. From #6,975,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [2:33:08, 127.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:39:29] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:41:21] [0] \t\t #> Saving chunk 280: \t 25,000 passages and 1,709,758 embeddings. From #7,000,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "281it [2:35:13, 126.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:41:34] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:43:27] [0] \t\t #> Saving chunk 281: \t 25,000 passages and 1,692,895 embeddings. From #7,025,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "282it [2:37:19, 126.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:43:40] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:45:32] [0] \t\t #> Saving chunk 282: \t 25,000 passages and 1,710,759 embeddings. From #7,050,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "283it [2:39:24, 126.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:45:46] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:47:38] [0] \t\t #> Saving chunk 283: \t 25,000 passages and 1,511,934 embeddings. From #7,075,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [2:41:28, 125.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:47:49] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:49:45] [0] \t\t #> Saving chunk 284: \t 25,000 passages and 1,535,323 embeddings. From #7,100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "285it [2:43:35, 125.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:49:57] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:51:48] [0] \t\t #> Saving chunk 285: \t 25,000 passages and 1,533,023 embeddings. From #7,125,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "286it [2:45:39, 125.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:52:00] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:53:50] [0] \t\t #> Saving chunk 286: \t 25,000 passages and 1,537,226 embeddings. From #7,150,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [2:47:41, 124.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:54:02] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:55:54] [0] \t\t #> Saving chunk 287: \t 25,000 passages and 1,539,311 embeddings. From #7,175,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "288it [2:49:45, 124.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:56:06] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 04:57:58] [0] \t\t #> Saving chunk 288: \t 25,000 passages and 1,540,607 embeddings. From #7,200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "289it [2:51:49, 124.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 04:58:10] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:00:01] [0] \t\t #> Saving chunk 289: \t 25,000 passages and 1,542,025 embeddings. From #7,225,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "290it [2:53:52, 123.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:00:14] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:02:06] [0] \t\t #> Saving chunk 290: \t 25,000 passages and 1,528,317 embeddings. From #7,250,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "291it [2:55:56, 123.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:02:18] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:04:13] [0] \t\t #> Saving chunk 291: \t 25,000 passages and 1,533,825 embeddings. From #7,275,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "292it [2:58:04, 125.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:04:25] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:06:18] [0] \t\t #> Saving chunk 292: \t 25,000 passages and 1,689,834 embeddings. From #7,300,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "293it [3:00:10, 125.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:06:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:08:30] [0] \t\t #> Saving chunk 293: \t 25,000 passages and 1,698,563 embeddings. From #7,325,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "294it [3:02:22, 127.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:08:44] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:10:36] [0] \t\t #> Saving chunk 294: \t 25,000 passages and 1,718,991 embeddings. From #7,350,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "295it [3:04:28, 127.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:10:49] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:12:42] [0] \t\t #> Saving chunk 295: \t 25,000 passages and 1,690,309 embeddings. From #7,375,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "296it [3:06:34, 126.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:12:55] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:14:47] [0] \t\t #> Saving chunk 296: \t 25,000 passages and 1,661,092 embeddings. From #7,400,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "297it [3:08:39, 126.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:15:00] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:16:52] [0] \t\t #> Saving chunk 297: \t 25,000 passages and 1,639,392 embeddings. From #7,425,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "298it [3:10:44, 125.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:17:05] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:19:00] [0] \t\t #> Saving chunk 298: \t 25,000 passages and 1,632,067 embeddings. From #7,450,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "299it [3:12:51, 126.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:19:12] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:21:04] [0] \t\t #> Saving chunk 299: \t 25,000 passages and 1,641,432 embeddings. From #7,475,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [3:14:56, 125.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:21:17] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:23:09] [0] \t\t #> Saving chunk 300: \t 25,000 passages and 1,645,104 embeddings. From #7,500,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [3:17:00, 125.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:23:22] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:25:13] [0] \t\t #> Saving chunk 301: \t 25,000 passages and 1,641,316 embeddings. From #7,525,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "302it [3:19:05, 125.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:25:26] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:27:18] [0] \t\t #> Saving chunk 302: \t 25,000 passages and 1,637,531 embeddings. From #7,550,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303it [3:21:10, 125.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:27:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:29:23] [0] \t\t #> Saving chunk 303: \t 25,000 passages and 1,643,504 embeddings. From #7,575,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "304it [3:23:15, 124.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:29:36] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:31:30] [0] \t\t #> Saving chunk 304: \t 25,000 passages and 1,626,861 embeddings. From #7,600,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "305it [3:25:22, 125.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:31:43] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:33:38] [0] \t\t #> Saving chunk 305: \t 25,000 passages and 1,635,874 embeddings. From #7,625,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "306it [3:27:30, 126.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:33:51] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:35:43] [0] \t\t #> Saving chunk 306: \t 25,000 passages and 1,627,608 embeddings. From #7,650,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "307it [3:29:34, 125.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:35:55] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:37:48] [0] \t\t #> Saving chunk 307: \t 25,000 passages and 1,648,350 embeddings. From #7,675,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "308it [3:31:39, 125.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:38:00] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:39:53] [0] \t\t #> Saving chunk 308: \t 25,000 passages and 1,632,317 embeddings. From #7,700,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "309it [3:33:45, 125.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:40:06] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:41:57] [0] \t\t #> Saving chunk 309: \t 25,000 passages and 1,598,199 embeddings. From #7,725,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "310it [3:35:48, 124.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:42:10] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:44:04] [0] \t\t #> Saving chunk 310: \t 25,000 passages and 1,614,174 embeddings. From #7,750,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "311it [3:37:55, 125.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:44:17] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:46:11] [0] \t\t #> Saving chunk 311: \t 25,000 passages and 1,621,472 embeddings. From #7,775,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "312it [3:40:03, 126.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:46:24] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:48:18] [0] \t\t #> Saving chunk 312: \t 25,000 passages and 1,629,172 embeddings. From #7,800,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [3:42:10, 126.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:48:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:50:26] [0] \t\t #> Saving chunk 313: \t 25,000 passages and 1,624,339 embeddings. From #7,825,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "314it [3:44:18, 126.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:50:39] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:52:33] [0] \t\t #> Saving chunk 314: \t 25,000 passages and 1,623,129 embeddings. From #7,850,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "315it [3:46:25, 126.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:52:46] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:54:40] [0] \t\t #> Saving chunk 315: \t 25,000 passages and 1,624,853 embeddings. From #7,875,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "316it [3:48:32, 126.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:54:53] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:56:50] [0] \t\t #> Saving chunk 316: \t 25,000 passages and 1,599,553 embeddings. From #7,900,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317it [3:50:42, 127.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:57:03] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 05:58:56] [0] \t\t #> Saving chunk 317: \t 25,000 passages and 1,614,856 embeddings. From #7,925,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [3:52:48, 127.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 05:59:09] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:01:03] [0] \t\t #> Saving chunk 318: \t 25,000 passages and 1,623,521 embeddings. From #7,950,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "319it [3:54:55, 127.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:01:16] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:03:10] [0] \t\t #> Saving chunk 319: \t 25,000 passages and 1,663,334 embeddings. From #7,975,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "320it [3:57:02, 127.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:03:24] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:05:18] [0] \t\t #> Saving chunk 320: \t 25,000 passages and 1,652,960 embeddings. From #8,000,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "321it [3:59:10, 127.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:05:31] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:07:26] [0] \t\t #> Saving chunk 321: \t 25,000 passages and 1,507,369 embeddings. From #8,025,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "322it [4:01:17, 127.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:07:38] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:09:32] [0] \t\t #> Saving chunk 322: \t 25,000 passages and 1,537,845 embeddings. From #8,050,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "323it [4:03:23, 126.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:09:44] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:11:38] [0] \t\t #> Saving chunk 323: \t 25,000 passages and 1,537,948 embeddings. From #8,075,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "324it [4:05:30, 126.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:11:51] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:13:45] [0] \t\t #> Saving chunk 324: \t 25,000 passages and 1,521,111 embeddings. From #8,100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "325it [4:07:36, 126.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:13:57] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:15:52] [0] \t\t #> Saving chunk 325: \t 25,000 passages and 1,534,216 embeddings. From #8,125,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "326it [4:09:43, 126.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:16:04] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:17:59] [0] \t\t #> Saving chunk 326: \t 25,000 passages and 1,536,540 embeddings. From #8,150,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "327it [4:11:50, 126.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:18:11] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:20:07] [0] \t\t #> Saving chunk 327: \t 25,000 passages and 1,536,345 embeddings. From #8,175,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "328it [4:13:58, 127.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:20:19] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:22:14] [0] \t\t #> Saving chunk 328: \t 25,000 passages and 1,499,206 embeddings. From #8,200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "329it [4:16:05, 127.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:22:26] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:24:21] [0] \t\t #> Saving chunk 329: \t 25,000 passages and 1,617,581 embeddings. From #8,225,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "330it [4:18:13, 127.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:24:34] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:26:29] [0] \t\t #> Saving chunk 330: \t 25,000 passages and 1,694,783 embeddings. From #8,250,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "331it [4:20:22, 127.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:26:43] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:28:38] [0] \t\t #> Saving chunk 331: \t 25,000 passages and 1,706,698 embeddings. From #8,275,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "332it [4:22:30, 128.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:28:52] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:30:47] [0] \t\t #> Saving chunk 332: \t 25,000 passages and 1,729,656 embeddings. From #8,300,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "333it [4:24:39, 128.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:31:01] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:32:56] [0] \t\t #> Saving chunk 333: \t 25,000 passages and 1,685,556 embeddings. From #8,325,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "334it [4:26:48, 128.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:33:09] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:35:05] [0] \t\t #> Saving chunk 334: \t 25,000 passages and 1,649,418 embeddings. From #8,350,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "335it [4:28:57, 128.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:35:18] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:37:14] [0] \t\t #> Saving chunk 335: \t 25,000 passages and 1,659,287 embeddings. From #8,375,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "336it [4:31:06, 128.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:37:27] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:39:20] [0] \t\t #> Saving chunk 336: \t 25,000 passages and 1,648,295 embeddings. From #8,400,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "337it [4:33:12, 127.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:39:33] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:41:26] [0] \t\t #> Saving chunk 337: \t 25,000 passages and 1,653,879 embeddings. From #8,425,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [4:35:18, 127.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:41:39] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:43:31] [0] \t\t #> Saving chunk 338: \t 25,000 passages and 1,653,740 embeddings. From #8,450,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "339it [4:37:22, 126.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:43:44] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:45:36] [0] \t\t #> Saving chunk 339: \t 25,000 passages and 1,643,083 embeddings. From #8,475,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "340it [4:39:27, 125.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:45:48] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:47:42] [0] \t\t #> Saving chunk 340: \t 25,000 passages and 1,646,388 embeddings. From #8,500,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "341it [4:41:34, 126.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:47:55] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:49:50] [0] \t\t #> Saving chunk 341: \t 25,000 passages and 1,640,056 embeddings. From #8,525,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "342it [4:43:42, 126.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:50:03] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:51:59] [0] \t\t #> Saving chunk 342: \t 25,000 passages and 1,640,150 embeddings. From #8,550,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343it [4:45:51, 127.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:52:12] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:54:09] [0] \t\t #> Saving chunk 343: \t 25,000 passages and 1,641,148 embeddings. From #8,575,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "344it [4:48:01, 128.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:54:22] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:56:17] [0] \t\t #> Saving chunk 344: \t 25,000 passages and 1,650,802 embeddings. From #8,600,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "345it [4:50:09, 128.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:56:30] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 06:58:22] [0] \t\t #> Saving chunk 345: \t 25,000 passages and 1,637,012 embeddings. From #8,625,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "346it [4:52:14, 127.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 06:58:35] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 07:00:29] [0] \t\t #> Saving chunk 346: \t 25,000 passages and 1,603,532 embeddings. From #8,650,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "347it [4:54:21, 127.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:00:42] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 07:02:35] [0] \t\t #> Saving chunk 347: \t 25,000 passages and 1,606,696 embeddings. From #8,675,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "348it [4:56:26, 126.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:02:47] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 07:04:40] [0] \t\t #> Saving chunk 348: \t 25,000 passages and 1,618,378 embeddings. From #8,700,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "349it [4:58:31, 126.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:04:52] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 07:06:44] [0] \t\t #> Saving chunk 349: \t 25,000 passages and 1,611,407 embeddings. From #8,725,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "350it [5:00:35, 125.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:06:57] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 07:08:49] [0] \t\t #> Saving chunk 350: \t 25,000 passages and 1,611,775 embeddings. From #8,750,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "351it [5:02:40, 125.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:09:01] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 07:10:54] [0] \t\t #> Saving chunk 351: \t 25,000 passages and 1,630,933 embeddings. From #8,775,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "352it [5:04:45, 125.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:11:06] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 08, 07:13:02] [0] \t\t #> Saving chunk 352: \t 25,000 passages and 1,621,965 embeddings. From #8,800,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "353it [5:06:54, 126.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:13:15] [0] \t\t #> Encoding 16823 passages..\n",
      "[Oct 08, 07:14:33] [0] \t\t #> Saving chunk 353: \t 16,823 passages and 1,086,283 embeddings. From #8,825,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "354it [5:08:20, 52.26s/it] \n",
      " 11%|█         | 39/354 [00:00<00:00, 385.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:14:41] [0] \t\t #> Checking all files were saved...\n",
      "[Oct 08, 07:14:41] [0] \t\t Found all files!\n",
      "[Oct 08, 07:14:41] [0] \t\t #> Building IVF...\n",
      "[Oct 08, 07:14:41] [0] \t\t #> Loading codes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 354/354 [00:00<00:00, 390.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:14:42] [0] \t\t Sorting codes...\n",
      "[Oct 08, 07:15:53] [0] \t\t Getting unique codes...\n",
      "[Oct 08, 07:15:54] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Oct 08, 07:15:54] #> Building the emb2pid mapping..\n",
      "[Oct 08, 07:16:12] len(emb2pid) = 597760090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 262144/262144 [01:41<00:00, 2581.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:18:02] #> Saved optimized IVF to /home/ubuntu/splade-colBERT/ColBERT/experiments/msmarco/indexes/msmarco.nbits=2/ivf.pid.pt\n",
      "[Oct 08, 07:18:02] [0] \t\t #> Saving the indexing metadata to /home/ubuntu/splade-colBERT/ColBERT/experiments/msmarco/indexes/msmarco.nbits=2/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\n",
    "    indexer = Indexer(checkpoint=r\"experiments/msmarco/colbert\", config=config)\n",
    "    indexer.index(name=\"msmarco.nbits=2\", collection=r\"../data/collection.tsv\", overwrite='resume')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc5111-ad78-49e4-9946-e06c1ebaecc8",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fb5fa49-90d6-4f78-8775-217341f69168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert.data import Queries\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f9bab25-4456-4580-b3a2-889bb8bae256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColBERTConfig(ncells=None, centroid_score_threshold=None, ndocs=None, index_path=None, nbits=1, kmeans_niters=4, resume=False, similarity='cosine', bsize=64, accumsteps=1, lr=3e-06, maxsteps=500000, save_every=None, warmup=None, warmup_bert=None, relu=False, nway=2, use_ib_negatives=False, reranker=False, distillation_alpha=1.0, ignore_scores=False, query_maxlen=32, attend_to_mask_tokens=False, interaction='colbert', dim=128, doc_maxlen=220, mask_punctuation=True, checkpoint='experiments/msmarco/colbert', triples=None, collection='../data/collection.tsv', queries=None, index_name='msmarco.nbits=2', overwrite=False, root='experiments', experiment='msmarco', index_root=None, name='', rank=0, nranks=1, amp=True, gpus=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef0198f1-300e-4ad7-9501-f8680456325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:33:26] #> Loading collection...\n",
      "0M 1M 2M 3M 4M 5M 6M 7M 8M \n",
      "[Oct 08, 07:33:53] #> Loading codec...\n",
      "[Oct 08, 07:33:53] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 08, 07:33:53] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 08, 07:33:53] #> Loading IVF...\n",
      "[Oct 08, 07:33:54] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:00<00:00, 810.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:33:55] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [01:30<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 07:35:26] #> Loading the queries from ../data/queries.dev.tsv ...\n",
      "[Oct 08, 07:35:26] #> Got 101093 queries. All QIDs are unique.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 101093/101093 [38:39<00:00, 43.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Oct 08, 08:15:08] #> Creating directory /home/ubuntu/splade-colBERT/ColBERT/experiments/msmarco/none/2022-10/08/01.51.47 \n",
      "\n",
      "\n",
      "[Oct 08, 08:15:25] #> Saved ranking of 101093 queries and 10109300 lines to /home/ubuntu/splade-colBERT/ColBERT/experiments/msmarco/none/2022-10/08/01.51.47/msmarco.nbits=2.ranking.tsv\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\n",
    "    searcher = Searcher(index=\"msmarco.nbits=2\", config=config)\n",
    "    queries = Queries(\"../data/queries.dev.tsv\")\n",
    "    ranking = searcher.search_all(queries, k=100)\n",
    "    ranking.save(\"msmarco.nbits=2.ranking.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df732cc-e414-4ba9-8c8a-5ea006388af8",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "454f1a38-9873-4912-8ba1-f4bf4b9f4fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 08:19:57] #> Loading QRELs from ../data/qrels.train.tsv ..\n",
      "#> Reading ../data/qrels.train.tsv\n",
      " 95%|█████████████▎| 9.5908842086792/10.09896469116211 [00:01<00:00,  7.66MiB/s]\n",
      "[Oct 08, 08:19:58] #> Loading ranked lists from experiments/msmarco/none/2022-10/08/01.51.47/msmarco.nbits=2.ranking.tsv ..\n",
      "#> Reading experiments/msmarco/none/2022-10/08/01.51.47/msmarco.nbits=2.ranking.tsv\n",
      "100%|██████████| 261.58467292785645/261.58467292785645 [00:17<00:00, 15.02MiB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/colbert/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ubuntu/miniconda3/envs/colbert/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/splade-colBERT/ColBERT/utility/evaluate/msmarco_passages.py\", line 126, in <module>\n",
      "    main(args)\n",
      "  File \"/home/ubuntu/splade-colBERT/ColBERT/utility/evaluate/msmarco_passages.py\", line 44, in main\n",
      "    assert set.issubset(set(qid2ranking.keys()), set(qid2positives.keys()))\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "!python -m utility.evaluate.msmarco_passages \\\n",
    "--ranking \"experiments/msmarco/none/2022-10/08/01.51.47/msmarco.nbits=2.ranking.tsv\" \\\n",
    "--qrels \"../data/qrels.train.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c70b10c-bdce-4b38-987a-441c09ca6762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 08, 08:22:48] #> Loading QRELs from ../data/qrels.dev.tsv ..\n",
      "#> Reading ../data/qrels.dev.tsv\n",
      " 95%|██████████▍| 1.089432716369629/1.1459598541259766 [00:00<00:00,  7.07MiB/s]\n",
      "[Oct 08, 08:22:48] #> Loading ranked lists from experiments/msmarco/none/2022-10/08/01.51.47/msmarco.nbits=2.ranking.tsv ..\n",
      "#> Reading experiments/msmarco/none/2022-10/08/01.51.47/msmarco.nbits=2.ranking.tsv\n",
      "100%|██████████| 261.58467292785645/261.58467292785645 [00:17<00:00, 14.94MiB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/colbert/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ubuntu/miniconda3/envs/colbert/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/splade-colBERT/ColBERT/utility/evaluate/msmarco_passages.py\", line 126, in <module>\n",
      "    main(args)\n",
      "  File \"/home/ubuntu/splade-colBERT/ColBERT/utility/evaluate/msmarco_passages.py\", line 44, in main\n",
      "    assert set.issubset(set(qid2ranking.keys()), set(qid2positives.keys()))\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "!python -m utility.evaluate.msmarco_passages \\\n",
    "--ranking \"experiments/msmarco/none/2022-10/08/01.51.47/msmarco.nbits=2.ranking.tsv\" \\\n",
    "--qrels \"../data/qrels.dev.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7abe06-c9d2-4785-8dfa-474150f4b684",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Correcting Triple Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0f68c2-508f-4024-8e7e-33d0c8f4c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert.evaluation.loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614a8213-ab05-40b3-92bd-62b4844821c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a78c298f-13d1-4908-9e34-82909c380324",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'triples': '../data/triples.train.small.tsv',\n",
    "    'queries': '../data/queries.train.tsv',\n",
    "    'collection': '../data/collection.tsv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a9b816-97c2-425d-8a1f-381584f8ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(queries_path):\n",
    "    queries = OrderedDict()\n",
    "\n",
    "    print_message(\"#> Loading the queries from\", queries_path, \"...\")\n",
    "\n",
    "    with open(queries_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            qid, query, *_ = line.replace(\"\\xa0\", \" \").strip().split('\\t')\n",
    "            qid = int(qid)\n",
    "\n",
    "            assert (qid not in queries), (\"Query QID\", qid, \"is repeated!\")\n",
    "            queries[re.sub('[^ 0-9a-zA-Z_-]', '', query.strip(\" \"))] = qid\n",
    "\n",
    "    print_message(\"#> Got\", len(queries), \"queries. All QIDs are unique.\\n\")\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c49dcf-46a7-4085-852f-816b288c3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_collection(collection_path):\n",
    "    print_message(\"#> Loading collection...\")\n",
    "\n",
    "    collection = {}\n",
    "\n",
    "    with open(collection_path, encoding=\"utf-8\") as f:\n",
    "        for line_idx, line in enumerate(f):\n",
    "            if line_idx % (1000*1000) == 0:\n",
    "                print(f'{line_idx // 1000 // 1000}M', end=' ', flush=True)\n",
    "\n",
    "            pid, passage, *rest = line.strip('\\n\\r ').split('\\t')\n",
    "            assert pid == 'id' or int(pid) == line_idx\n",
    "\n",
    "            if len(rest) >= 1:\n",
    "                title = rest[0]\n",
    "                passage = title + ' | ' + passage\n",
    "\n",
    "            collection[passage] = line_idx\n",
    "\n",
    "    print()\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a72a85-fff0-41c7-af74-72a12066136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af172c-d378-47c5-8137-a8f75a3cb953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 26, 18:41:38] #> Loading the queries from ../data/queries.train.tsv ...\n",
      "[Sep 26, 18:41:39] #> Got 806349 queries. All QIDs are unique.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = load_queries(param['queries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fe3e6-2310-49ab-a67c-f7b233839eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 26, 18:41:39] #> Loading collection...\n",
      "0M 1M 2M 3M 4M 5M 6M 7M 8M \n"
     ]
    }
   ],
   "source": [
    "collection = load_collection(param['collection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b7dcc-581a-4bfd-855f-39539d37d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global err_text\n",
    "err_text = \"\"\n",
    "def get_id(text, data):\n",
    "    err_text = text\n",
    "    _id = data.get(exceptions.get(text, text), None)\n",
    "    if _id is None: \n",
    "        text = text.replace(\"\\xa0\",' ')\n",
    "        _id = data.get(exceptions.get(text, text), None)\n",
    "    if _id is None: \n",
    "        n_text = text.strip(' ')\n",
    "        _id = data.get(exceptions.get(n_text, n_text), None)\n",
    "    if _id is None: \n",
    "        n_text = re.sub('[^ 0-9a-zA-Z_-]', '', n_text)\n",
    "        _id = data.get(exceptions.get(n_text, n_text), None)\n",
    "    if _id is None: \n",
    "        _id = data.get(text.strip(' '), None)\n",
    "    \n",
    "    if _id is None:\n",
    "        print(text)\n",
    "        raise Exception(text)\n",
    "    return _id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036987e-b802-4cb2-b409-b4ec10c631a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = {'divorce et sÃ©paration': 'divorce et séparation',\n",
    " 'what is intelÂ® vpro technology': 'what is intel® vpro technology',\n",
    " 'what is aÂ\\xa0shock wave': 'what is a shock wave',\n",
    " 'Germanyâ\\x80\\x99s perspective, the Treaty of Versailles was a fair settlement for its national interests': 'Germany’s perspective, the Treaty of Versailles was a fair settlement for its national interests',\n",
    " 'yesÃ¼n temÃ¼r khan emperor taiding of yuan': 'yesün temür khan emperor taiding of yuan',\n",
    " ' The vitamin that prevents beriberi is ': ' The vitamin that prevents beriberi is',\n",
    " ' phosphates as food ingredients ': ' phosphates as food ingredients',\n",
    " ' who invented the periodic table ': ' who invented the periodic table',\n",
    " 'what does bokmÃ¥l mean': 'what does bokmål mean',\n",
    " 'which action should youÂ\\xa0never take when selecting quotations': 'which action should you never take when selecting quotations',\n",
    " 'dermatitis, anemia, convulsions, depressions, and confusion are all signs of a vitamin _________Â\\xa0deficiency.': 'dermatitis anemia convulsions depressions and confusion are all signs of a vitamin _________ deficiency',\n",
    " ' In humans, the normal set point for body temperature is ': 'In humans the normal set point for body temperature is',\n",
    " 'what did you notice about the relationship between pressure and volume when the temperatureÂ\\xa0 is held constant?': 'what did you notice about the relationship between pressure and volume when the temperature  is held constant',\n",
    " 'the Â\\xa0____________Â\\xa0 that vibrates like a drum when sound waves hit.': 'the  ____________  that vibrates like a drum when sound waves hit',\n",
    " 'what is composition?Â\\xa0 why is composition important?': 'what is composition  why is composition important',\n",
    " 'the lithosphere consists of Â\\xa0____________': 'the lithosphere consists of  ____________',\n",
    " \"what is a 'cost engineer \": 'what is a cost engineer',\n",
    " 'A simple way to save with a competitive interest rate. Your Personal Savings account earns interest daily and is posted to your account monthly. You can easily set up recurring transfers from your current bank accounts to your Personal Savings account.â\\x80\\xa0. Just deposit your savings and watch it grow. Your Personal Savings account earns interest daily and is posted to your account monthly. You can easily set up recurring transfers from your current bank accounts to your Personal Savings account.â\\x80\\xa0. ': 'A simple way to save with a competitive interest rate. Your Personal Savings account earns interest daily and is posted to your account monthly. You can easily set up recurring transfers from your current bank accounts to your Personal Savings account.â\\x80\\xa0. Just deposit your savings and watch it grow. Your Personal Savings account earns interest daily and is posted to your account monthly. You can easily set up recurring transfers from your current bank accounts to your Personal Savings account.â\\x80\\xa0.',\n",
    " \"Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sira. Islamic jurisprudence is a complimentary expansion of the former by Islamic juris efinition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir \": \"Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sira. Islamic jurisprudence is a complimentary expansion of the former by Islamic juris efinition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir\",\n",
    " \"Definition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir efinition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir \": \"Definition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir efinition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe9c35-bc7f-478f-a820-0f6a8e976e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633d34783d8b4ddc8e9869a7c94fef16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = []\n",
    "with open(param['triples'], encoding='utf-8') as f:\n",
    "    for line in tqdm(f):\n",
    "        q_str, p_str_p, p_str_n = line.strip('\\n').split('\\t')\n",
    "        qid = get_id(q_str, queries)\n",
    "        pid_p = get_id(p_str_p, collection)\n",
    "        pid_n = get_id(p_str_n, collection)\n",
    "        example = [qid, pid_p, pid_n]\n",
    "        examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b055dee-54c8-4c5a-8071-3f07f7239c00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m output_path \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m----> 9\u001b[0m print_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#> Saved examples with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lines to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "id_file = '../data/triples.train.small.id.json'\n",
    "    \n",
    "with open(id_file, 'w') as f:\n",
    "    for example in examples:\n",
    "        ujson.dump(example, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "output_path = f.name\n",
    "print_message(f\"#> Saved examples with {len(self.data)} lines to {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9cf81-ebf7-41c3-9356-6c217e41c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_file = '../data/triples.train.small.id.tsv'\n",
    "with open(id_file, 'r') as f:\n",
    "    id_examples = ujson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7df61b-8835-4f85-b23a-b6350be7f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_examples)\n",
    "len(id_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1187d-c05b-495c-b796-98e35bffe2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sira. Islamic jurisprudence is a complimentary expansion of the former by Islamic juris efinition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir\",\n",
       " \"Definition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir efinition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir\"]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = collection\n",
    "# data = queries\n",
    "qid = [k for k in data.keys() if \"lamic Laws are made up of Shar\" in k]\n",
    "qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "cf666a92-1378-4abf-b4f1-32e94a42e6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divorce et sÃ©paration': 'divorce et séparation',\n",
       " 'what is intelÂ® vpro technology': 'what is intel® vpro technology',\n",
       " 'what is aÂ\\xa0shock wave': 'what is a shock wave',\n",
       " 'Germanyâ\\x80\\x99s perspective, the Treaty of Versailles was a fair settlement for its national interests': 'Germany’s perspective, the Treaty of Versailles was a fair settlement for its national interests',\n",
       " 'yesÃ¼n temÃ¼r khan emperor taiding of yuan': 'yesün temür khan emperor taiding of yuan',\n",
       " ' The vitamin that prevents beriberi is ': ' The vitamin that prevents beriberi is',\n",
       " ' phosphates as food ingredients ': ' phosphates as food ingredients',\n",
       " ' who invented the periodic table ': ' who invented the periodic table',\n",
       " 'what does bokmÃ¥l mean': 'what does bokmål mean',\n",
       " 'which action should youÂ\\xa0never take when selecting quotations': 'which action should you never take when selecting quotations',\n",
       " 'dermatitis, anemia, convulsions, depressions, and confusion are all signs of a vitamin _________Â\\xa0deficiency.': 'dermatitis anemia convulsions depressions and confusion are all signs of a vitamin _________ deficiency',\n",
       " ' In humans, the normal set point for body temperature is ': 'In humans the normal set point for body temperature is',\n",
       " 'what did you notice about the relationship between pressure and volume when the temperatureÂ\\xa0 is held constant?': 'what did you notice about the relationship between pressure and volume when the temperature  is held constant',\n",
       " 'the Â\\xa0____________Â\\xa0 that vibrates like a drum when sound waves hit.': 'the  ____________  that vibrates like a drum when sound waves hit',\n",
       " 'what is composition?Â\\xa0 why is composition important?': 'what is composition  why is composition important',\n",
       " 'the lithosphere consists of Â\\xa0____________': 'the lithosphere consists of  ____________',\n",
       " \"what is a 'cost engineer \": 'what is a cost engineer',\n",
       " 'A simple way to save with a competitive interest rate. Your Personal Savings account earns interest daily and is posted to your account monthly. You can easily set up recurring transfers from your current bank accounts to your Personal Savings account.â\\x80\\xa0. Just deposit your savings and watch it grow. Your Personal Savings account earns interest daily and is posted to your account monthly. You can easily set up recurring transfers from your current bank accounts to your Personal Savings account.â\\x80\\xa0. ': 'A simple way to save with a competitive interest rate. Your Personal Savings account earns interest daily and is posted to your account monthly. You can easily set up recurring transfers from your current bank accounts to your Personal Savings account.â\\x80\\xa0. Just deposit your savings and watch it grow. Your Personal Savings account earns interest daily and is posted to your account monthly. You can easily set up recurring transfers from your current bank accounts to your Personal Savings account.â\\x80\\xa0.',\n",
       " \"Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sira. Islamic jurisprudence is a complimentary expansion of the former by Islamic juris efinition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir \": \"Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sira. Islamic jurisprudence is a complimentary expansion of the former by Islamic juris efinition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir\",\n",
       " \"Definition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir efinition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir \": \"Definition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir efinition [edit]. Islamic Laws are made up of Shari'ah ('â\\x80\\x8eØ´Ø±Ù\\x8aØ¹Ø© Å\\xa0arÄ«Ê¿ah) and Islamic jurisprudence (Ù\\x81Ù\\x82Ù\\x87â\\x80\\x8e Fiqh). Shari'ah is seen as sacred and constitutes the Qur'an and Prophet Muhammad 's Sunnah (way), which is found in the Hadith and Sir\"}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exceptions = {**exceptions, **{p_str_n: qid[1]}}\n",
    "exceptions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
